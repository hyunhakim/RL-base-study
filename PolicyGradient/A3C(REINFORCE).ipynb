{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pylab\n",
    "import numpy as np\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\") \n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras import backend as K\n",
    "import gym\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "global episode\n",
    "episode = 0\n",
    "EPISODES = 500\n",
    "env_name = \"CartPole-v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A3CGobal:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = 1\n",
    "        self.actor_lr = 0.001\n",
    "        self.critic_lr = 0.005\n",
    "        self.discount_factor = 0.99\n",
    "        \n",
    "        # 스레드 갯수\n",
    "        self.num_thread = 2\n",
    "        \n",
    "        # Global_network의 actor /  critic\n",
    "        self.g_actor = self.build_actor()\n",
    "        self.g_critic = self.build_critic()\n",
    "        \n",
    "        self.g_actor_opt = self.actor_optimizer()\n",
    "        self.g_critic_opt = self.critic_optimizer()\n",
    "        \n",
    "    def build_actor(self):\n",
    "        input_layer = Input(shape=(self.state_size,))\n",
    "        x = Dense(24, activation='relu',\n",
    "                 kernel_initializer='he_uniform')(input_layer)\n",
    "        output = Dense(self.action_size, activation='softmax',\n",
    "                      kernel_initializer='he_uniform')(x)\n",
    "        \n",
    "        model = Model(inputs=input_layer, outputs=output)\n",
    "        \n",
    "        model._make_predict_function()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_critic(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_shape=(self.state_size, ), activation='relu',\n",
    "                       kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(12, activation='relu',\n",
    "                       kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.value_size, activation='linear',\n",
    "                       kernel_initializer='he_uniform'))\n",
    "        \n",
    "        model._make_predict_function()\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def actor_optimizer(self):\n",
    "        action = K.placeholder(shape=[None, self.action_size])\n",
    "        td_error = K.placeholder(shape=[None, ])\n",
    "        #discounted_rewards = K.placeholder(shape=[None, ])\n",
    "        \n",
    "        # 크로스 엔트로피 오류함수 계산\n",
    "        action_prob = K.sum(action * self.g_actor.output, axis=1)\n",
    "        cross_entropy = K.log(action_prob) * td_error\n",
    "        loss = -K.sum(cross_entropy)\n",
    "        \n",
    "        # 정책신경망을 업데이트하는 훈련함수 생성\n",
    "        optimizer = Adam(lr=self.actor_lr)\n",
    "        updates = optimizer.get_updates(self.g_actor.trainable_weights, [], loss)\n",
    "        train = K.function([self.g_actor.input, action, td_error], [],\n",
    "                           updates=updates)\n",
    "\n",
    "        return train\n",
    "    \n",
    "    def critic_optimizer(self):\n",
    "        target = K.placeholder(shape=[None, ])\n",
    "        loss = K.mean(K.square(target - self.g_critic.output))\n",
    "        optimizer = Adam(lr=self.critic_lr)\n",
    "        updates = optimizer.get_updates(self.g_critic.trainable_weights, [], loss)\n",
    "        train = K.function([self.g_critic.input, target], [],\n",
    "                           updates=updates)\n",
    "        \n",
    "        return train\n",
    "    \n",
    "    def train(self):\n",
    "        actor_learners = [ActorLearner(self.state_size, self.action_size, self.value_size,\n",
    "                                      self.g_actor, self.g_critic,\n",
    "                                      self.g_actor_opt, self.g_critic_opt,\n",
    "                                      self.discount_factor)\n",
    "                         for _ in range(self.num_thread)]\n",
    "        \n",
    "        # 각 스레드 시작\n",
    "        for actor_learner in actor_learners:\n",
    "            time.sleep(1)\n",
    "            actor_learner.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorLearner(threading.Thread):\n",
    "    def __init__(self, state_size, action_size, value_size, global_actor, global_critic,\n",
    "                g_actor_opt, g_critic_opt, discount_factor):\n",
    "        threading.Thread.__init__(self)\n",
    "        \n",
    "        self.discount_factor = discount_factor\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.value_size = value_size\n",
    "        \n",
    "        # Global_network의 actor /  critic\n",
    "        self.g_actor = global_actor\n",
    "        self.g_critic = global_critic\n",
    "        \n",
    "        self.g_actor_opt = g_actor_opt\n",
    "        self.g_critic_opt = g_critic_opt\n",
    "        \n",
    "        # local model\n",
    "        self.local_actor = self.build_local_actor()\n",
    "        self.local_critic = self.build_local_critic()\n",
    "        \n",
    "        # 지정된 타임스텝 동안 샘플을 저장할 리스트\n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "        \n",
    "        # 모델 업데이트 주기\n",
    "        self.t = 0\n",
    "        self.t_max = 20\n",
    "    \n",
    "    def build_local_actor(self):\n",
    "        input_layer = Input(shape=(self.state_size,))\n",
    "        x = Dense(24, activation='relu',\n",
    "                 kernel_initializer='he_uniform')(input_layer)\n",
    "        output = Dense(self.action_size, activation='softmax',\n",
    "                      kernel_initializer='he_uniform')(x)\n",
    "        \n",
    "        local_actor = Model(inputs=input_layer, outputs=output)\n",
    "        \n",
    "        local_actor._make_predict_function()\n",
    "        local_actor.set_weights(self.g_actor.get_weights())\n",
    "        \n",
    "        return local_actor\n",
    "    \n",
    "    def build_local_critic(self):\n",
    "        local_critic = Sequential()\n",
    "        local_critic.add(Dense(24, input_shape=(self.state_size, ), activation='relu',\n",
    "                       kernel_initializer='he_uniform'))\n",
    "        local_critic.add(Dense(12, activation='relu',\n",
    "                       kernel_initializer='he_uniform'))\n",
    "        local_critic.add(Dense(self.value_size, activation='linear',\n",
    "                       kernel_initializer='he_uniform'))\n",
    "        \n",
    "        local_critic._make_predict_function()\n",
    "        local_critic.set_weights(self.g_critic.get_weights())\n",
    "        \n",
    "        return local_critic\n",
    "    \n",
    "    def train_global_model(self):\n",
    "        \n",
    "        states = np.array(self.states)\n",
    "        actions = np.array(self.actions)\n",
    "        values = self.local_critic.predict(states)[0]\n",
    "        discounted_rewards = self.discount_rewards(self.rewards)\n",
    "        advantages = discounted_rewards - value\n",
    "        \n",
    "        self.g_actor_opt([states, actions, advantages])\n",
    "        self.g_critic_opt([states, discounted_rewards])\n",
    "        \n",
    "        self.states, self.actions, self.rewards = [], [], []\n",
    "    \n",
    "    def discount_rewards(self, rewards):\n",
    "        discounted_rewards = np.zeros_like(rewards)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(0, len(rewards))):\n",
    "            running_add = running_add * self.discount_factor + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "    \n",
    "    \n",
    "    def update_local_model(self):\n",
    "        self.local_actor.set_weights(self.g_actor.get_weights())\n",
    "        self.local_critic.set_weights(self.g_critic.get_weights())\n",
    "    \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        policy = self.local_actor.predict(state)[0]\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "    \n",
    "    \n",
    "    def append_sample(self, state, action, reward):\n",
    "        self.states.append(state[0])\n",
    "        self.rewards.append(reward)\n",
    "        act = np.zeros(self.action_size)\n",
    "        act[action] = 1\n",
    "        self.actions.append(act)\n",
    "    \n",
    "    def run(self):\n",
    "        global episode\n",
    "        env = gym.make(env_name)\n",
    "        \n",
    "        step = 0\n",
    "        \n",
    "        while episode < EPISODES:\n",
    "            done = False\n",
    "            score = 0\n",
    "            state = env.reset()\n",
    "            state = np.reshape(state, [1,4])\n",
    "\n",
    "            while not done:\n",
    "                #env.render()\n",
    "                print(\"!!!!\")\n",
    "                step += 1\n",
    "                self.t += 1\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1,4])\n",
    "                reward = reward if not done or score == 499 else -100\n",
    "                agent.append_sample(state, action, reward)\n",
    "\n",
    "                score += reward\n",
    "                state = next_state\n",
    "                \n",
    "                if self.t >= self.t_max or done:\n",
    "                    self.train_global_model()\n",
    "                    self.update_local_model()\n",
    "                    self.t = 0\n",
    "                    \n",
    "                if done:\n",
    "                    episode += 1\n",
    "                    score = score if score == 500.0 else score + 100\n",
    "                    print(\"Episode: \", episode, \" Score: \", score, \" step: \", step)\n",
    "                    step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-56:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-51-7d0aa3a18687>\", line 113, in run\n",
      "    action = self.get_action(state)\n",
      "  File \"<ipython-input-51-7d0aa3a18687>\", line 85, in get_action\n",
      "    policy = self.local_actor.predict(state)[0]\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1462, in predict\n",
      "    callbacks=callbacks)\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\", line 324, in predict_loop\n",
      "    batch_outs = f(ins_batch)\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3073, in __call__\n",
      "    self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3019, in _make_callable\n",
      "    callable_fn = session._make_callable_from_options(callable_opts)\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1471, in _make_callable_from_options\n",
      "    return BaseSession._Callable(self, callable_options)\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1425, in __init__\n",
      "    session._session, options_ptr, status)\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Tensor input_53:0, specified in either feed_devices or fetch_devices was not found in the Graph\n",
      "\n",
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x00000128C40B88D0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: No such callable handle: 1274014990408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-57:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-51-7d0aa3a18687>\", line 113, in run\n",
      "    action = self.get_action(state)\n",
      "  File \"<ipython-input-51-7d0aa3a18687>\", line 85, in get_action\n",
      "    policy = self.local_actor.predict(state)[0]\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1462, in predict\n",
      "    callbacks=callbacks)\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\", line 324, in predict_loop\n",
      "    batch_outs = f(ins_batch)\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3073, in __call__\n",
      "    self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3019, in _make_callable\n",
      "    callable_fn = session._make_callable_from_options(callable_opts)\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1471, in _make_callable_from_options\n",
      "    return BaseSession._Callable(self, callable_options)\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1425, in __init__\n",
      "    session._session, options_ptr, status)\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: Tensor input_54:0, specified in either feed_devices or fetch_devices was not found in the Graph\n",
      "\n",
      "Exception ignored in: <bound method BaseSession._Callable.__del__ of <tensorflow.python.client.session.BaseSession._Callable object at 0x00000128B8332240>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1455, in __del__\n",
      "    self._session._session, self._handle, status)\n",
      "  File \"C:\\Users\\gusgk\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 528, in __exit__\n",
      "    c_api.TF_GetCode(self.status.status))\n",
      "tensorflow.python.framework.errors_impl.InvalidArgumentError: No such callable handle: 1274014990408\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    global_agent = A3CGobal(state_size=4, action_size=2)\n",
    "    global_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00674005, -0.04818258, -0.01514307,  0.02286146])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "state1 = env.reset()\n",
    "state1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(4,))\n",
    "x = Dense(24, activation='relu',\n",
    "         kernel_initializer='he_uniform')(input_layer)\n",
    "output = Dense(2, activation='softmax',\n",
    "              kernel_initializer='he_uniform')(x)\n",
    "\n",
    "model1 = Model(inputs=input_layer, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "state1 = np.reshape(state1, [1,4])\n",
    "a=model1.predict(state1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(2, 1, p=a)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
