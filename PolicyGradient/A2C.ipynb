{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pylab\n",
    "import numpy as np\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\") \n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras import backend as K\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A2C Agent\n",
    " - init\n",
    " - build_actor\n",
    " - build_critic\n",
    " - train_actor\n",
    " - actor_optimizer\n",
    " - train_critic\n",
    " - critic_optimizer\n",
    " - append_sample\n",
    " - get_action\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    def __init__(self):\n",
    "        self.state_size = 4\n",
    "        self.action_size = 2\n",
    "        \n",
    "        self.actor = self.build_actor()\n",
    "        \n",
    "    def build_actor(self):\n",
    "        input_layer = Input(shape(self.state_size,))\n",
    "        x = Dense(24, activation='relu')(input_layer)\n",
    "        x = Dense(12, activation='relu')(x)\n",
    "        output = Dense(self.action_size, activation='softmax')\n",
    "        \n",
    "        model = Model(inputs=input_layer, outputs=output)\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def actor_optimizer(self):\n",
    "        action = K.placeholder(shape=[None, self.action_size])\n",
    "        td_error = K.placeholder(shape=[None, ])\n",
    "        #discounted_rewards = K.placeholder(shape=[None, ])\n",
    "        \n",
    "        # 크로스 엔트로피 오류함수 계산\n",
    "        action_prob = K.sum(action * self.actor.output, axis=1)\n",
    "        cross_entropy = K.log(action_prob) * td_error\n",
    "        loss = -K.sum(cross_entropy)\n",
    "        \n",
    "        # 정책신경망을 업데이트하는 훈련함수 생성\n",
    "        optimizer = Adam(lr=self.learning_rate)\n",
    "        updates = optimizer.get_updates(self.actor.trainable_weights, [], loss)\n",
    "        train = K.function([self.model.input, action, td_error], [],\n",
    "                           updates=updates)\n",
    "\n",
    "        return train\n",
    "        \n",
    "    def train_actor(self, state, action, next_state):\n",
    "        td_error = self.td_error(state, next_state)\n",
    "        self.actor_optimizer([state, action, td_error])\n",
    "        \n",
    "    def build_critic(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_shape=(self.state_size, ), activation='relu'))\n",
    "        model.add(Dense(12, activation='linear'))\n",
    "        \n",
    "        model.compile(loss='mse', optimizer='adam')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def critic_optimizer(self):\n",
    "        \n",
    "        \n",
    "    def train_critic(self):\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        policy = self.actor.predict(state)[0]\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "    \n",
    "    def td_error(self, state, next_state):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
